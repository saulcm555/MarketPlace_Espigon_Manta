# PILAR 3: MCP - CHATBOT MULTIMODAL
## Estado de ImplementaciÃ³n y Roadmap Detallado

---

## âš ï¸ ACLARACIONES ARQUITECTÃ“NICAS CRÃTICAS

### 1ï¸âƒ£ NO MEZCLAR AI ORCHESTRATOR CON MCP SERVICE

**AI Orchestrator (Puerto 3004):**
- **ROL:** ğŸ§  Cerebro - Toma decisiones de IA
- **Responsabilidades:**
  - Recibe mensajes del usuario (texto, PDF)
  - Procesa con LLM (Gemini) para entender intenciÃ³n
  - Decide QUÃ‰ tools ejecutar y con QUÃ‰ parÃ¡metros
  - Construye respuesta final coherente para el usuario
  - Maneja contexto/historial de conversaciones
- **NO hace:** Llamadas directas a Payment/Rest/Report Service

**MCP Service (Puerto 3003):**
- **ROL:** âš™ï¸ Ejecutor - NO piensa, solo ejecuta
- **Responsabilidades:**
  - Expone MCP Tools como endpoints HTTP
  - Ejecuta llamadas a microservicios (Payment, Rest, Report)
  - Valida parÃ¡metros de entrada
  - Formatea respuestas para el LLM
  - Maneja errores de microservicios
- **NO hace:** Decidir quÃ© tool ejecutar ni generar respuestas de IA

**Flujo Correcto:**
```
Usuario â†’ AI Orchestrator â†’ LLM (Gemini) â†’ Decide tool â†’ 
MCP Service â†’ Ejecuta tool â†’ Microservicio â†’ Respuesta â†’ 
MCP Service â†’ Formatea â†’ LLM â†’ Respuesta final â†’ Usuario
```

### 2ï¸âƒ£ ENDPOINTS VERIFICADOS âœ…

Todos los endpoints necesarios para las 5 MCP Tools existen:
- âœ… `buscar_productos` â†’ `GET /api/products` (Rest Service)
- âœ… `crear_orden` â†’ `POST /api/orders` (Rest Service)
- âœ… `resumen_ventas` â†’ GraphQL `top_sellers_report` (Report Service)
- âœ… `procesar_pago` â†’ `POST /api/payments/process` (Payment Service)
- âœ… `consultar_pago` â†’ `GET /api/payments/transaction/:id` (Payment Service)

**NO necesitas crear endpoints nuevos en otros servicios.**

### 3ï¸âƒ£ MULTIMODAL MÃNIMO: TEXTO + PDF ÃšNICAMENTE

Para cumplir el requisito de "mÃ­nimo 2 tipos de entrada":
- âœ… **Texto:** Mensajes normales del usuario
- âœ… **PDF:** Facturas, catÃ¡logos, reportes (con pdf-parse)
- ğŸš« **NO OCR:** Tesseract es complejo y menos confiable
- ğŸš« **NO ImÃ¡genes:** Requiere Gemini Vision + vector DB (fase posterior)

---

## ğŸ“‹ RESUMEN EJECUTIVO

### Â¿QuÃ© es el Pilar 3?
El Pilar 3 es el **Chatbot Inteligente Multimodal** que permite a usuarios interactuar con el marketplace usando lenguaje natural y documentos PDF. Utiliza:
- **Model Context Protocol (MCP)**: Protocolo estÃ¡ndar para exponer funciones del backend como "tools" ejecutables por LLMs
- **LLM (Gemini)**: Motor de procesamiento de lenguaje natural para entender intenciones y generar respuestas
- **Procesamiento Multimodal**: Capacidad de procesar texto + PDFs (imÃ¡genes en fase posterior)

### Estado Actual: 30% Completado
âœ… **Completado:**
- Payment Service preparado con contratos y autenticaciÃ³n
- Estructura base del `mcp_service/` con 2 tools funcionales
- PaymentClient HTTP con autenticaciÃ³n automÃ¡tica
- Event constants y DTOs compartidos

âŒ **Pendiente:**
- AI Orchestrator (nÃºcleo del sistema)
- LLM Adapter con Gemini
- 3 MCP Tools adicionales
- Procesamiento PDF (Ãºnico multimodal inicial)
- Chat UI o integraciÃ³n con Telegram

---

## ğŸ—ï¸ ARQUITECTURA DEL PILAR 3

### ğŸ”‘ SEPARACIÃ“N DE RESPONSABILIDADES (CRÃTICO)

**AI ORCHESTRATOR (Puerto 3004)**
- Recibe mensajes del usuario (texto, PDF)
- Llama al LLM (Gemini) para entender intenciÃ³n
- Decide quÃ© MCP Tools ejecutar
- Construye respuesta final coherente
- **NO ejecuta lÃ³gica de negocio directamente**

**MCP SERVICE (Puerto 3003)**
- Expone MCP Tools como funciones ejecutables
- Ejecuta llamadas HTTP a microservicios (Payment, Rest, Report)
- Formatea respuestas de tools para el LLM
- **NO toma decisiones de IA, solo ejecuta**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FRONTEND                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Chat UI     â”‚              â”‚  Telegram    â”‚            â”‚
â”‚  â”‚  (React)     â”‚              â”‚  (n8n)       â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚         â”‚                              â”‚                    â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP/WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             AI ORCHESTRATOR (Puerto 3004)                   â”‚
â”‚             ğŸ§  CEREBRO - TOMA DECISIONES                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  ChatController                                     â”‚    â”‚
â”‚  â”‚  - POST /api/chat/message                          â”‚    â”‚
â”‚  â”‚  - POST /api/chat/multimodal (text + PDFs)         â”‚    â”‚
â”‚  â”‚  - GET  /api/chat/history/:userId                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LLMService (Strategy Pattern)                      â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚   â”‚
â”‚  â”‚  â”‚GeminiAdapterâ”‚ (Ãºnica implementaciÃ³n inicial)       â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  MCPClient (Cliente HTTP)                           â”‚   â”‚
â”‚  â”‚  - Llama a MCP Service para ejecutar tools          â”‚   â”‚
â”‚  â”‚  - Recibe resultados de tools                       â”‚   â”‚
â”‚  â”‚  - Pasa resultados al LLM para continuar            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                 â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  PDFProcessor (Procesamiento Multimodal)            â”‚   â”‚
â”‚  â”‚  - Extrae texto de PDFs con pdf-parse               â”‚   â”‚
â”‚  â”‚  - Agrega texto extraÃ­do al contexto del LLM        â”‚   â”‚
â”‚  â”‚  - **SOLO PDF, no OCR ni imÃ¡genes (inicio)**       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ HTTP Calls (POST /tools/execute)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      MCP SERVICE (Puerto 3003) - âš™ï¸ EJECUTOR                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  MCP Tools (Funciones ejecutables por LLM)         â”‚    â”‚
â”‚  â”‚  âœ… procesar_pago (Payment Service)                â”‚    â”‚
â”‚  â”‚  âœ… consultar_pago (Payment Service)               â”‚    â”‚
â”‚  â”‚  âŒ buscar_productos (Rest Service - VERIFICADO)   â”‚    â”‚
â”‚  â”‚  âŒ crear_orden (Rest Service - VERIFICADO)        â”‚    â”‚
â”‚  â”‚  âŒ resumen_ventas (Report Service - VERIFICADO)   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                â”‚                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  HTTP Clients (AutenticaciÃ³n automÃ¡tica)           â”‚    â”‚
â”‚  â”‚  âœ… PaymentClient â†’ Payment Service                â”‚    â”‚
â”‚  â”‚  âŒ ProductClient â†’ Rest Service                   â”‚    â”‚
â”‚  â”‚  âŒ OrderClient â†’ Rest Service                     â”‚    â”‚
â”‚  â”‚  âŒ ReportClient â†’ Report Service                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MICROSERVICIOS EXISTENTES                      â”‚
â”‚  âœ… Payment Service (3001)  âœ… Rest Service (3002)         â”‚
â”‚  âœ… Report Service (3005)   âœ… Realtime Service (3006)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš§ COMPONENTES PENDIENTES (Detalle TÃ©cnico)

### 1ï¸âƒ£ AI ORCHESTRATOR SERVICE (CRÃTICO - NÃºcleo del Pilar 3)

#### DescripciÃ³n
Servicio central que coordina todo el procesamiento de IA. Recibe mensajes del usuario, los procesa con LLMs, ejecuta MCP Tools necesarios, y devuelve respuestas coherentes.

#### Estructura de Archivos
```
backend/ai_orchestrator/
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â””â”€â”€ src/
    â”œâ”€â”€ main.ts                          # Entry point, Express server
    â”œâ”€â”€ config/
    â”‚   â””â”€â”€ env.ts                       # Variables de entorno
    â”œâ”€â”€ controllers/
    â”‚   â””â”€â”€ ChatController.ts            # Endpoints HTTP
    â”œâ”€â”€ services/
    â”‚   â”œâ”€â”€ LLMService.ts               # LÃ³gica de procesamiento LLM
    â”‚   â”œâ”€â”€ MCPService.ts               # Ejecutor de MCP Tools
    â”‚   â””â”€â”€ ConversationService.ts      # Manejo de contexto/historial
    â”œâ”€â”€ adapters/
    â”‚   â”œâ”€â”€ LLMAdapter.ts               # Interface Strategy Pattern
    â”‚   â””â”€â”€ GeminiAdapter.ts            # ImplementaciÃ³n Gemini (Ãºnica inicial)
    â”œâ”€â”€ processors/
    â”‚   â””â”€â”€ PDFProcessor.ts             # Procesamiento de PDFs (Ãºnico multimodal)
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ Conversation.ts             # Modelo de conversaciÃ³n
    â”‚   â””â”€â”€ Message.ts                  # Modelo de mensaje
    â””â”€â”€ utils/
        â”œâ”€â”€ promptBuilder.ts            # Constructor de prompts
        â””â”€â”€ responseFormatter.ts        # Formateador de respuestas
```

#### Endpoints a Implementar
```typescript
// POST /api/chat/message - Mensaje (texto + opcional PDF)
// FORMATO: FormData (no JSON)
// Campos:
//   - userId: string
//   - message: string
//   - conversationId?: string (null = nueva conversaciÃ³n)
//   - files?: File[] (PDFs opcionales)

Example Request (FormData):
  userId: "uuid"
  message: "Busca productos de electrÃ³nica bajo $100"
  conversationId: "uuid-123" (opcional)
  files: [File] (opcional, solo PDFs)

â†’ Response: {
  "conversationId": "uuid",
  "response": "EncontrÃ© 5 productos...",
  "toolsUsed": ["buscar_productos"],
  "confidence": 0.95,
  "extractedText": "..." // solo si hay PDFs
}

// GET /api/chat/history/:userId - Historial de conversaciones
â†’ Response: {
  "conversations": [
    {"id": "uuid", "createdAt": "timestamp", "messages": [...]}
  ]
}
```

#### Dependencias Clave
```json
{
  "dependencies": {
    "@google/generative-ai": "^0.21.0",      // SDK Gemini
    "axios": "^1.7.0",                       // HTTP calls a MCP Service
    "express": "^4.18.0",                    // Server HTTP
    "pdf-parse": "^1.1.1",                   // Parseo de PDFs (Ãºnico multimodal)
    "multer": "^1.4.5",                      // Upload de archivos
    "ioredis": "^5.3.0"                     // Cache de conversaciones (opcional)
  }
}

// âŒ NO instalar: openai, tesseract.js, sharp (no necesarios en MVP)
```

#### Decisiones ArquitectÃ³nicas a Tomar
1. **Â¿Persistir conversaciones en PostgreSQL o Redis?**
   - PostgreSQL: BÃºsquedas complejas, analytics
   - Redis: Ultra-rÃ¡pido, temporal, buen cache
   - **RecomendaciÃ³n**: PostgreSQL + Redis como cache

2. **Â¿Streaming de respuestas (tipo ChatGPT) o respuesta completa?**
   - Streaming: Mejor UX, mÃ¡s complejo (WebSocket/SSE)
   - Completa: MÃ¡s simple, espera total
   - **RecomendaciÃ³n**: Empezar con respuesta completa, agregar streaming despuÃ©s

3. **Â¿LÃ­mite de tokens por conversaciÃ³n?**
   - Gemini Pro: 32k tokens contexto
   - **RecomendaciÃ³n**: Mantener Ãºltimos 10 mensajes + system prompt

4. **Â¿Retry logic en caso de fallo del LLM?**
   - **RecomendaciÃ³n**: 3 reintentos con exponential backoff

---

### 2ï¸âƒ£ LLM ADAPTER (CRÃTICO - Cerebro del Sistema)

#### DescripciÃ³n
ImplementaciÃ³n del patrÃ³n Strategy para soportar mÃºltiples proveedores de LLM (Gemini, OpenAI, Claude). Permite cambiar de proveedor sin afectar el resto del cÃ³digo.

#### Interface Base
```typescript
// src/adapters/LLMAdapter.ts
export interface LLMAdapter {
  generateResponse(params: {
    messages: Message[];
    tools: MCPTool[];
    temperature?: number;
    maxTokens?: number;
  }): Promise<LLMResponse>;

  generateMultimodalResponse(params: {
    textPrompt: string;
    images: Buffer[];
    documents: ProcessedDocument[];
    tools: MCPTool[];
  }): Promise<LLMResponse>;

  streamResponse(params: {
    messages: Message[];
    tools: MCPTool[];
  }): AsyncGenerator<string, void, unknown>;
}

export interface LLMResponse {
  text: string;
  toolCalls?: ToolCall[];
  finishReason: 'stop' | 'length' | 'tool_calls';
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

export interface ToolCall {
  toolName: string;
  arguments: Record<string, any>;
}
```

#### ImplementaciÃ³n Gemini (Principal)
```typescript
// src/adapters/GeminiAdapter.ts
import { GoogleGenerativeAI, FunctionDeclarationSchemaType } from '@google/generative-ai';

export class GeminiAdapter implements LLMAdapter {
  private genAI: GoogleGenerativeAI;
  private model: any;

  constructor(apiKey: string, modelName: string = 'gemini-2.0-flash') {
    this.genAI = new GoogleGenerativeAI(apiKey);
    this.model = this.genAI.getGenerativeModel({ model: modelName });
  }

  async generateResponse(params: {
    messages: Message[];
    tools: MCPTool[];
    temperature?: number;
  }): Promise<LLMResponse> {
    // Convertir MCP Tools a formato Gemini Function Calling
    const functionDeclarations = params.tools.map(tool => ({
      name: tool.name,
      description: tool.description,
      parameters: {
        type: FunctionDeclarationSchemaType.OBJECT,
        properties: tool.parameters.properties,
        required: tool.parameters.required
      }
    }));

    // Construir prompt con historial de mensajes
    const chat = this.model.startChat({
      history: this.convertMessagesToGemini(params.messages),
      tools: [{ functionDeclarations }],
      generationConfig: {
        temperature: params.temperature ?? 0.7,
        maxOutputTokens: 2048
      }
    });

    const result = await chat.sendMessage(params.messages[params.messages.length - 1].content);
    const response = result.response;

    // Detectar si hay function calls
    const toolCalls = this.extractFunctionCalls(response);
    
    return {
      text: response.text() || '',
      toolCalls,
      finishReason: toolCalls.length > 0 ? 'tool_calls' : 'stop',
      usage: {
        promptTokens: response.usageMetadata?.promptTokenCount || 0,
        completionTokens: response.usageMetadata?.candidatesTokenCount || 0,
        totalTokens: response.usageMetadata?.totalTokenCount || 0
      }
    };
  }

  async generateMultimodalResponse(params: {
    textPrompt: string;
    images: Buffer[];
    documents: ProcessedDocument[];
  }): Promise<LLMResponse> {
    // Usar gemini-2.0-flash para multimodal
    const visionModel = this.genAI.getGenerativeModel({ model: 'gemini-2.0-flash' });

    // Construir contenido multimodal
    const parts = [
      { text: params.textPrompt }
    ];

    // Agregar imÃ¡genes
    for (const imageBuffer of params.images) {
      parts.push({
        inlineData: {
          data: imageBuffer.toString('base64'),
          mimeType: 'image/jpeg'
        }
      });
    }

    // Agregar texto extraÃ­do de documentos
    for (const doc of params.documents) {
      parts.push({ text: `[Documento ${doc.name}]:\n${doc.extractedText}` });
    }

    const result = await visionModel.generateContent(parts);
    const response = result.response;

    return {
      text: response.text(),
      finishReason: 'stop',
      usage: {
        promptTokens: 0, // Gemini no expone mÃ©tricas en multimodal
        completionTokens: 0,
        totalTokens: 0
      }
    };
  }

  private convertMessagesToGemini(messages: Message[]) {
    return messages.slice(0, -1).map(msg => ({
      role: msg.role === 'user' ? 'user' : 'model',
      parts: [{ text: msg.content }]
    }));
  }

  private extractFunctionCalls(response: any): ToolCall[] {
    const functionCalls = response.functionCalls();
    if (!functionCalls) return [];

    return functionCalls.map((fc: any) => ({
      toolName: fc.name,
      arguments: fc.args
    }));
  }
}
```

#### Preguntas Clave
1. **Â¿Usar Gemini 2.0 Flash Pro**
   - Flash: 1M tokens/min, $0.075/1M tokens
   - Pro: 2M tokens/min, $1.25/1M tokens
   - **RecomendaciÃ³n**: Flash para producciÃ³n, Pro para testing

2. **Â¿Implementar OpenAI como fallback?**
   - Pros: Mayor disponibilidad, menor latencia en ciertos paÃ­ses
   - Contras: MÃ¡s caro ($10/1M tokens), requiere otra API key
   - **RecomendaciÃ³n**: Solo Gemini inicialmente, agregar OpenAI si hay problemas de disponibilidad

3. **Â¿System Prompt global o personalizado por usuario?**
   - Global: MÃ¡s simple, comportamiento uniforme
   - Personalizado: Mejor UX, adapta tono/idioma
   - **RecomendaciÃ³n**: System Prompt global + parÃ¡metros por usuario (idioma, rol)

---

### 3ï¸âƒ£ MCP TOOLS ADICIONALES (Alta Prioridad)

#### Tool 3: buscar_productos âœ… ENDPOINT VERIFICADO

**Endpoint Real:** `GET /api/products` (Rest Service)  
**Filtros disponibles:** search, id_category, id_sub_category, id_seller, min_price, max_price, page, limit

```typescript
// mcp_service/src/tools/buscar_productos.ts
import { MCPTool } from '../types/MCPTool';
import { ProductClient } from '../clients/ProductClient';

export const buscar_productos: MCPTool = {
  name: 'buscar_productos',
  description: 'Busca productos en el marketplace por nombre, categorÃ­a o precio. Usa el endpoint GET /api/products del Rest Service.',
  parameters: {
    type: 'object',
    properties: {
      search: {
        type: 'string',
        description: 'TÃ©rmino de bÃºsqueda (nombre o descripciÃ³n del producto)'
      },
      id_category: {
        type: 'string',
        description: 'ID de la categorÃ­a (UUID)'
      },
      id_seller: {
        type: 'string',
        description: 'ID del vendedor (UUID)'
      },
      min_price: {
        type: 'number',
        description: 'Precio mÃ­nimo en dÃ³lares'
      },
      max_price: {
        type: 'number',
        description: 'Precio mÃ¡ximo en dÃ³lares'
      },
      page: {
        type: 'number',
        description: 'NÃºmero de pÃ¡gina (default: 1)'
      },
      limit: {
        type: 'number',
        description: 'Productos por pÃ¡gina (default: 10)'
      }
    },
    required: [] // Todos opcionales
  },
  
  async execute(args: {
    search?: string;
    id_category?: string;
    id_seller?: string;
    min_price?: number;
    max_price?: number;
    page?: number;
    limit?: number;
  }) {
    const client = new ProductClient();
    
    // Llamar a Rest Service: GET /api/products con query params
    const response = await client.listProducts(args);

    return {
      success: true,
      products: response.products.map(p => ({
        id: p.product_id,
        name: p.product_name,
        description: p.description,
        price: p.price,
        stock: p.stock
      })),
      totalFound: response.pagination.totalItems,
      currentPage: response.pagination.page
    };
  },

  formatResponse(result: any): string {
    if (!result.success || result.products.length === 0) {
      return 'No encontrÃ© productos con esos criterios.';
    }

    let response = `EncontrÃ© ${result.totalFound} producto(s):\n\n`;
    for (const product of result.products) {
      response += `ğŸ“¦ **${product.name}**\n`;
      response += `   ğŸ’° Precio: $${product.price}\n`;
      response += `   ğŸ“Š Stock: ${product.stock} unidades\n\n`;
    }
    return response;
  }
};
```

**Cliente HTTP Necesario:**
```typescript
// mcp_service/src/clients/ProductClient.ts
import axios, { AxiosInstance } from 'axios';
import { env } from '../config/env';

export class ProductClient {
  private client: AxiosInstance;

  constructor() {
    this.client = axios.create({
      baseURL: env.REST_SERVICE_URL, // http://localhost:3002
      headers: {
        'X-Internal-Api-Key': env.INTERNAL_API_KEY,
        'Content-Type': 'application/json'
      },
      timeout: 10000
    });
  }

  // \u00daNICO M\u00c9TODO: GET /api/products con query params
  async listProducts(params: {
    search?: string;
    id_category?: string;
    id_seller?: string;
    min_price?: number;
    max_price?: number;
    page?: number;
    limit?: number;
  }) {
    const response = await this.client.get('/api/products', { params });
    return response.data; // { products: [], pagination: {} }
  }

  async getProduct(productId: string) {
    const response = await this.client.get(`/api/products/${productId}`);
    return response.data;
  }
}
```

#### Tool 4: crear_orden âœ… ENDPOINT VERIFICADO

**Endpoint Real:** `POST /api/orders` (Rest Service)  
**AutenticaciÃ³n:** ğŸ”’ Requiere Bearer token (rol `client`)

### âœ… AUTH RESUELTA: USAR TOKEN DEMO FIJO (OpciÃ³n B)

El endpoint `POST /api/orders` requiere un Bearer token del cliente.

**DECISIÃ“N TOMADA:** Usar **OpciÃ³n B (Token Demo)** para desarrollo.
- Crear un usuario client de prueba en Auth Service
- Hacer login y copiar su Bearer token al .env del MCP Service
- MÃ¡s adelante migrar a OpciÃ³n A (token del usuario real desde frontend)

**ImplementaciÃ³n OpciÃ³n B (Demo):**
```typescript
// mcp_service/.env
DEMO_CLIENT_TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...

// mcp_service/src/clients/OrderClient.ts
export class OrderClient {
  constructor(userToken?: string) {
    this.client = axios.create({
      baseURL: env.REST_SERVICE_URL,
      headers: {
        'Authorization': `Bearer ${userToken || env.DEMO_CLIENT_TOKEN}`,
        'Content-Type': 'application/json'
      }
    });
  }
}
```

---

// mcp_service/src/tools/crear_orden.ts
export const crear_orden: MCPTool = {
  name: 'crear_orden',
  description: 'Crea una orden de compra. Requiere autenticaciÃ³n del cliente. Endpoint: POST /api/orders del Rest Service.',
  parameters: {
    type: 'object',
    properties: {
      customerId: {
        type: 'string',
        description: 'ID del cliente que realiza la orden'
      },
      products: {
        type: 'array',
        description: 'Lista de productos a comprar',
        items: {
          type: 'object',
          properties: {
            productId: { type: 'string' },
            quantity: { type: 'number' }
          },
          required: ['productId', 'quantity']
        }
      },
      shippingAddress: {
        type: 'object',
        description: 'DirecciÃ³n de envÃ­o',
        properties: {
          street: { type: 'string' },
          city: { type: 'string' },
          state: { type: 'string' },
          zipCode: { type: 'string' }
        },
        required: ['street', 'city', 'state', 'zipCode']
      }
    },
    required: ['customerId', 'products', 'shippingAddress']
  },

  async execute(args: any) {
    const orderClient = new OrderClient();
    
    // POST /api/orders
    const order = await orderClient.createOrder({
      customer_id: args.customerId,
      items: args.products,
      shipping_address: args.shippingAddress,
      status: 'pending'
    });

    return {
      success: true,
      orderId: order.order_id,
      total: order.total_amount,
      status: order.status,
      estimatedDelivery: order.estimated_delivery
    };
  },

  formatResponse(result: any): string {
    return `âœ… Orden creada exitosamente!\n` +
           `ğŸ†” ID: ${result.orderId}\n` +
           `ğŸ’µ Total: $${result.total}\n` +
           `ğŸ“¦ Estado: ${result.status}\n` +
           `ğŸšš Entrega estimada: ${result.estimatedDelivery}`;
  }
};
```

#### Tool 5: resumen_ventas âœ… QUERY VERIFICADO

**Query Real:** `top_sellers_report` (Report Service - GraphQL)  
**Endpoint:** `POST /graphql`

```typescript
// mcp_service/src/tools/resumen_ventas.ts
export const resumen_ventas: MCPTool = {
  name: 'resumen_ventas',
  description: 'Obtiene un resumen de ventas de los top vendedores. Usa el query top_sellers_report del Report Service (GraphQL).',
  parameters: {
    type: 'object',
    properties: {
      startDate: {
        type: 'string',
        description: 'Fecha inicio del perÃ­odo (YYYY-MM-DD)'
      },
      endDate: {
        type: 'string',
        description: 'Fecha fin del perÃ­odo (YYYY-MM-DD)'
      },
      limit: {
        type: 'number',
        description: 'NÃºmero de top vendedores (default: 5)'
      }
    },
    required: ['startDate', 'endDate']
  },

  async execute(args: {
    startDate: string;
    endDate: string;
    limit?: number;
  }) {
    const reportClient = new ReportClient();
    
    // GraphQL Query
    const query = `
      query TopSellers($startDate: String!, $endDate: String!, $limit: Int) {
        top_sellers_report(
          date_range: { start_date: $startDate, end_date: $endDate }
          limit: $limit
        ) {
          top_sellers {
            seller_id
            seller_name
            total_sales
            total_orders
          }
        }
      }
    `;

    const response = await reportClient.executeGraphQLQuery(query, {
      startDate: args.startDate,
      endDate: args.endDate,
      limit: args.limit || 5
    });

    return {
      success: true,
      topSellers: response.data.top_sellers_report.top_sellers
    };
  },

  formatResponse(result: any): string {
    let response = `ğŸ“Š **Top Vendedores**\n\n`;
    for (const seller of result.topSellers) {
      response += `ğŸ† ${seller.seller_name}\n`;
      response += `   ğŸ’° Ventas totales: $${seller.total_sales}\n`;
      response += `   ğŸ“¦ Ã“rdenes: ${seller.total_orders}\n\n`;
    }
    return response;
  }
};
```

**Pregunta Clave:**
- **Â¿El Rest Service ya tiene estos endpoints implementados?**
  - Verificar en [rest_service/readmes/API_ENDPOINTS.md](backend/rest_service/readmes/API_ENDPOINTS.md)
  - Si no existen, se deben crear PRIMERO en Rest Service antes de implementar estos tools

---

### 4ï¸âƒ£ PROCESAMIENTO MULTIMODAL (Media Prioridad)

#### âš ï¸ REQUISITO MÃNIMO: TEXTO + 1 TIPO ADICIONAL

**DecisiÃ³n:** Implementar **TEXTO + PDF** (mÃ¡s estable que OCR)

**RazÃ³n:**
- pdf-parse es mÃ¡s confiable que Tesseract.js
- Menos dependencias nativas
- Casos de uso claros: catÃ¡logos, facturas, reportes
- Se puede agregar OCR despuÃ©s si es necesario

#### PDF Processor (ÃšNICO PROCESADOR MULTIMODAL INICIAL)
```typescript
// ai_orchestrator/src/processors/PDFProcessor.ts
import pdfParse from 'pdf-parse';

export interface ProcessedDocument {
  name: string;
  extractedText: string;
  pageCount: number;
  metadata: any;
}

export class PDFProcessor {
  async extractTextFromPDF(pdfBuffer: Buffer, filename: string): Promise<ProcessedDocument> {
    const data = await pdfParse(pdfBuffer);

    return {
      name: filename,
      extractedText: data.text,
      pageCount: data.numpages,
      metadata: data.info
    };
  }

  async extractTextBatch(pdfs: Array<{name: string, data: Buffer}>): Promise<ProcessedDocument[]> {
    return Promise.all(pdfs.map(pdf => this.extractTextFromPDF(pdf.data, pdf.name)));
  }
}
```

**Casos de Uso:**
- Usuario sube catÃ¡logo PDF â†’ Extrae lista de productos
- Usuario sube factura â†’ Extrae datos de compra
- Usuario sube reporte â†’ Genera resumen automÃ¡tico

**ğŸ“Œ IMPORTANTE:** NO implementar OCR ni anÃ¡lisis de imÃ¡genes inicialmente. Si despuÃ©s necesitas bÃºsqueda visual, se puede agregar Gemini Vision en una fase posterior.

---

### 5ï¸âƒ£ CHAT UI / INTERFAZ DE USUARIO (Baja Prioridad - Puede ser Pilar 4)

#### OpciÃ³n A: Chat Component en React (Frontend)
```
frontend/src/components/ChatBot/
â”œâ”€â”€ ChatWindow.tsx           # Contenedor principal
â”œâ”€â”€ MessageList.tsx          # Lista de mensajes con scroll
â”œâ”€â”€ MessageInput.tsx         # Input con soporte multimodal
â”œâ”€â”€ FileUpload.tsx           # Drag & drop para imÃ¡genes/PDFs
â”œâ”€â”€ TypingIndicator.tsx      # "AI estÃ¡ escribiendo..."
â””â”€â”€ chatbot.css
```

**Endpoints Frontend:**
```typescript
// frontend/src/api/chat.ts
export const sendMessage = async (params: {
  userId: string;
  message: string;
  conversationId?: string;
  files?: File[];
}) => {
  const formData = new FormData();
  formData.append('userId', params.userId);
  formData.append('message', params.message);
  if (params.conversationId) {
    formData.append('conversationId', params.conversationId);
  }
  params.files?.forEach(file => formData.append('files', file));

  const response = await fetch('http://localhost:3004/api/chat/message', {
    method: 'POST',
    body: formData
  });

  return response.json();
};
```

#### OpciÃ³n B: Telegram Bot (via n8n)
```
n8n Workflow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Telegram Triggerâ”‚ â†’ Detecta mensajes/imÃ¡genes de usuarios
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Extract User ID â”‚ â†’ Identifica usuario de Telegram
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HTTP Request    â”‚ â†’ POST http://localhost:3004/api/chat/message
â”‚ (AI Orchestrator)â”‚   Body: { userId, message, images }
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Send Response   â”‚ â†’ EnvÃ­a respuesta del AI al chat de Telegram
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pregunta Clave:**
- **Â¿QuÃ© interfaz prefieres implementar primero?**
  - React Chat: Integrado en el marketplace, mejor UX, mÃ¡s control
  - Telegram Bot: MÃ¡s rÃ¡pido de implementar, accesible desde cualquier dispositivo
  - Ambos: MÃ¡xima flexibilidad pero mÃ¡s complejidad

---

## ğŸ“Š MATRIZ DE PRIORIDADES

| Componente | Prioridad | Esfuerzo | Bloqueante para | Estado |
|------------|-----------|----------|-----------------|--------|
| AI Orchestrator | ğŸ”´ CRÃTICO | 3 dÃ­as | Todo el Pilar 3 | âŒ Pendiente |
| LLM Adapter (Gemini) | ğŸ”´ CRÃTICO | 2 dÃ­as | Todo el Pilar 3 | âŒ Pendiente |
| buscar_productos Tool | ğŸŸ¡ ALTA | 4 horas | BÃºsqueda conversacional | âœ… Endpoint verificado |
| crear_orden Tool | ğŸŸ¡ ALTA | 4 horas | Compra conversacional | âœ… Endpoint verificado |
| resumen_ventas Tool | ğŸŸ¡ ALTA | 4 horas | Analytics conversacional | âœ… Query verificado |
| PDF Processor | ğŸŸ¢ MEDIA | 6 horas | Multimodal (mÃ­nimo) | âŒ Pendiente |
| Chat UI (React) | ğŸ”µ BAJA | 2 dÃ­as | UX final | âŒ Pendiente |
| Telegram Bot | ğŸ”µ BAJA | 1 dÃ­a | UX alternativa | âŒ Pendiente |

**ğŸš« NO IMPLEMENTAR (fuera del MVP):**
- OCR Processor (Tesseract) - Complejidad innecesaria
- Image Analyzer (Gemini Vision) - Requiere vector DB
- OpenAI Adapter - Solo Gemini inicialmente

---

## ğŸ¯ PLAN DE IMPLEMENTACIÃ“N SUGERIDO

### Fase 1: NÃºcleo Funcional (Semana 1)
```
DÃA 1-2: AI Orchestrator Service
- Setup proyecto (package.json, tsconfig, estructura)
- Implementar ChatController bÃ¡sico
- Implementar LLMService con Gemini
- Implementar MCPService (ejecutor de tools)

DÃA 3: LLM Adapter
- Implementar GeminiAdapter completo
- Pruebas de function calling con tools existentes
- Manejo de errores y retries

DÃA 4-5: MCP Tools Adicionales
- Implementar ProductClient + buscar_productos
- Implementar OrderClient + crear_orden
- Implementar ReportClient + resumen_ventas
- Testing end-to-end
```

### Fase 2: Multimodalidad MÃNIMA (Semana 2)
```
DÃA 6: PDF Processor ÃšNICO
- PDFProcessor con pdf-parse (6 horas)
- Testing con facturas/catÃ¡logos reales

DÃA 7-8: IntegraciÃ³n Multimodal
- Endpoint POST /api/chat/multimodal
- Manejo de archivos PDF (multer)
- ValidaciÃ³n MIME: application/pdf Ãºnicamente
- Agregado de texto extraÃ­do al contexto LLM
- Testing end-to-end: texto + PDF â†’ respuesta

DÃA 9: Buffer/Refinamiento
- Fix de bugs encontrados
- Mejoras de prompts
- DocumentaciÃ³n
```

### Fase 3: UI (Semana 3 - Opcional)
```
DÃA 10-12: Chat Component React
- ChatWindow con manejo de estado
- MessageList con renderizado dinÃ¡mico
- FileUpload con drag & drop
- IntegraciÃ³n con AI Orchestrator

ALTERNATIVA: Telegram Bot (1 dÃ­a)
- Workflow n8n
- Manejo de comandos /start, /help
- EnvÃ­o/recepciÃ³n de imÃ¡genes
```

---

## ğŸ”§ CONFIGURACIÃ“N NECESARIA

### Variables de Entorno
```bash
# ai_orchestrator/.env
PORT=3004
NODE_ENV=development

# Gemini API
GEMINI_API_KEY=tu_api_key_aquÃ­
GEMINI_MODEL=gemini-2.0-flash

# MCP Service
MCP_SERVICE_URL=http://localhost:3003
INTERNAL_API_KEY=shared_secret_key

# Microservicios
REST_SERVICE_URL=http://localhost:3002
PAYMENT_SERVICE_URL=http://localhost:3001
REPORT_SERVICE_URL=http://localhost:3005

# Base de datos (opcional - para persistir conversaciones)
DATABASE_URL=postgresql://user:pass@localhost:5432/marketplace

# Redis (opcional - cache de conversaciones)
REDIS_URL=redis://localhost:6379
```

### API Keys Necesarias
1. **Gemini API Key**
   - Ir a: https://ai.google.dev/
   - Crear proyecto en Google AI Studio
   - Generar API key
   - LÃ­mites gratis: 1500 requests/dÃ­a, 1M tokens/min

2. **OpenAI API Key (Opcional)**
   - Ir a: https://platform.openai.com/api-keys
   - Crear API key
   - LÃ­mites: Pay-as-you-go, $5 crÃ©dito inicial

---

## â“ PREGUNTAS CRÃTICAS ANTES DE EMPEZAR

### Decisiones de Producto
1. Â¿QuÃ© idiomas debe soportar el chatbot? (EspaÃ±ol, InglÃ©s, ambos)
2. Â¿Los usuarios podrÃ¡n realizar compras completas vÃ­a chat? (bÃºsqueda â†’ orden â†’ pago)
3. Â¿Se necesita memoria a largo plazo de las conversaciones? (sÃ­ = PostgreSQL, no = Redis temporal)
4. Â¿QuÃ© rol debe tener el chatbot? (asistente de compras, soporte tÃ©cnico, ambos)

### Decisiones TÃ©cnicas
5. Â¿Usar Gemini Flash (econÃ³mico) o Pro (potente)? **â†’ RECOMENDADO: Flash**
6. Â¿Implementar streaming de respuestas o respuestas completas? **â†’ RECOMENDADO: Completas (mÃ¡s simple)**
7. Â¿Persistir conversaciones en DB o solo en memoria? **â†’ RECOMENDADO: PostgreSQL + Redis cache**
8. Â¿Implementar rate limiting por usuario? **â†’ SÃ (opcional pero recomendado)**
9. Â¿Prefieren React Chat UI o Telegram Bot como primera interfaz? **â†’ Decidir segÃºn UX deseada**

### âš ï¸ ValidaciÃ³n de Infraestructura
10. Â¿El Rest Service ya tiene endpoints de bÃºsqueda de productos? **âœ… SÃ: GET /api/products con filtros**
11. Â¿El Rest Service tiene endpoints de creaciÃ³n de Ã³rdenes? **âœ… SÃ: POST /api/orders (requiere Bearer token)**
12. Â¿El Report Service tiene endpoint de resumen de ventas? **âœ… SÃ: query GraphQL top_sellers_report**
13. Â¿Todos los microservicios aceptan autenticaciÃ³n con `X-Internal-Api-Key`? **âš ï¸ VERIFICAR:**
    - âœ… Payment Service: Ya configurado
    - â“ Rest Service: Revisar si tiene middleware de X-Internal-Api-Key
    - â“ Report Service: Revisar si tiene middleware de X-Internal-Api-Key
    - **Si NO tienen:** Opciones:
      - Agregar middleware en Rest/Report (recomendado)
      - Usar Bearer token de servicio
      - Dejar endpoints pÃºblicos temporalmente (solo demo)

---

## ğŸ“š RECURSOS Y DOCUMENTACIÃ“N

### DocumentaciÃ³n Oficial
- [Gemini Function Calling Guide](https://ai.google.dev/docs/function_calling)
- [Model Context Protocol Spec](https://modelcontextprotocol.io/introduction)
- [pdf-parse Documentation](https://www.npmjs.com/package/pdf-parse)

### Arquitectura Existente
- [Payment Service Integration Guide](backend/payment_service/docs/iniciopilar3.md)
- [Rest Service API Endpoints](backend/rest_service/readmes/API_ENDPOINTS.md)
- [Backend Architecture](backend/ARQUITECTURA_BACKEND_EXPLICADA.md)

### Patrones de DiseÃ±o Aplicados
- Strategy Pattern (LLM Adapters)
- Factory Pattern (Tool execution)
- Adapter Pattern (Payment providers)
- Saga Pattern (Eventual consistency)

---

## âœ… CHECKLIST DE PREPARACIÃ“N

Antes de empezar a codificar, asegÃºrate de:

- [ ] Tener Gemini API Key activa
- [ ] Verificar que Rest Service tenga endpoints necesarios
- [ ] Verificar que Report Service tenga endpoints necesarios
- [ ] Confirmar que todos los servicios usan `X-Internal-Api-Key`
- [ ] Decidir si usar PostgreSQL o Redis para conversaciones
- [ ] Decidir si implementar streaming o respuestas completas
- [ ] Decidir quÃ© UI implementar (React Chat o Telegram)
- [ ] Decidir si necesitan bÃºsqueda visual (vector DB)
- [ ] Leer la documentaciÃ³n de Gemini Function Calling
- [ ] Tener claro el System Prompt del chatbot (personalidad, capacidades)

---

## ğŸš€ COMANDO PARA EMPEZAR

Una vez respondidas las preguntas crÃ­ticas:

```bash
# Crear estructura del AI Orchestrator
cd backend
mkdir ai_orchestrator
cd ai_orchestrator
npm init -y
npm install express @google/generative-ai axios pdf-parse multer
npm install --save-dev typescript @types/node @types/express @types/multer ts-node nodemon

# Copiar tsconfig base
cp ../mcp_service/tsconfig.json .

# Crear estructura de carpetas (SIN OCRProcessor ni ImageAnalyzer)
mkdir -p src/{config,controllers,services,adapters,processors,models,utils}

# Iniciar desarrollo
npm run dev
```

---

## ğŸ“Œ NOTAS FINALES

- **No bloquea Pilar 4 (Event Bus)**: Puedes implementar Pilar 3 y 4 en paralelo
- **Iterativo**: Empieza con lo mÃ­nimo funcional (AI Orchestrator + Gemini + 2 tools)
- **Escalable**: La arquitectura permite agregar mÃ¡s tools sin cambiar el nÃºcleo
- **Testeable**: Cada componente es independiente y se puede probar aisladamente

**Este documento debe ser tu guÃ­a completa para implementar el Pilar 3. RevÃ­salo, haz preguntas especÃ­ficas sobre cualquier secciÃ³n, y empezamos cuando estÃ©s listo.** ğŸ¯
